# SOEN691Project
## Abstract
This report will perform analysis on a dataset which contains physical characters of poisonous or edible mushrooms. The dataset is Mushroom Data Set [1] from UCI machine learning repository provided by Jeff Schlimmer. First, we will preprocess this dataset to get the training set and testing set. Then use the training set to train a random forest classifier, a k-nearest neighbors classifier, and a naive bayes classifier in Spark, and try to predict whether a mushroom is safe or poisonous with using a customized cost matrix. Second, we will analyze the results and also compare the accuracy, precision, recall, F1 score and cost of time of these techniques. 
## Introduction
Mushrooms are a common ingredient in human food. However, some kinds of mushroom can cause severe, sometimes even fatal, food poisoning, and these accidents usually due to misidentification [2]. Therefore, this project targets to apply supervised machine learning techniques on poisonous mushroom identification. We will use a mushroom dataset from UCI Machine Learning Repository. It has physical characteristics about 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom. This data set has 22 features and four classes(edible, definitely poisonous, unknown edibility and not recommended). It contains 8124 instances and there are missing values in some of those instances. Verma, S. K., and M. Dutta have done a similar research on the same dataset using ANN and ANFIS algorithm [3], and Muhammad Husaini has evaluated Naive Bayes, RIDOR, and Bayes Net algorithms on this dataset [4]. <br/>
To add more comparisons, this project will use this dataset to train KNN, random forest, and naive bayes classifiers and do the prediction. We will use Spark since it supports parallelization to provide better training efficiency. Here are some problems we need to solve: How to select useful features? How to balance the data? How to deal with missing values? How to represent categorical data in numerical values to make it feasible for selected classifiers? And how to normalize the data? After this, we need to consider how to apply the methods provided by Spark on the data. Then, after we get the model, we need to use the model on the test set and analyze the results. At the end, we will conclude which model can achieve a better prediction.  
## Materials and Methods
We will use the Mushroom data set from UCI to train three different classifiers based on KNN, Random Forest and Naive Bayes algorithms. We are going to divide the whole process into three individual steps: Data Preprocessing, Training and Evaluation.  
### Data Preprocessing
As soon as we get the raw data. The very first one is preprocessing the dataset. There are six step-by-step methods in this part: Data Selection, Missing Values Imputation, Categorical Features Encoding, Data Normalization, Training and Test set separation and Data Balance.
#### Data Selection
There are 22 different physical features for this task while the information extracted from them varies a lot. I.e. Some of the features tell us more useful hints compared with the rest. We are going to select the most valuable features based on different standpoint and evaluate the impact of the different selections to the final results. Since this dataset has a large amount of features, we will also apply PCA techniques to reduce the dimension of the dataset. In addition, there are four different labels in the raw dataset which are definitely edible, definitely poisonous, or of unknown edibility and not recommended. Since we believe that the last two of them would disturb the classifier, we are going to discard the raws with these two labels at the beginning. 
#### Missing Values Imputation
As we know, real-world datasets often contain missing values which are not usually acceptable for most machine learning algorithms. After the Data Selection procedure, we are going to apply a substitution strategy for unknown feature values. Since there are lots of missing values in the original input matrix, the strategy handled this issue happens to be essential. We will use several different ways, e.g. throwing instances with missing values or imputing missing value with the most frequent value in that raw to deal with it and compare the result of them respectively. 
#### Categorical Features Encoding
We already get reasonable data that is fully filled. We are going to deal with categorical data issues. Since all of the data attributes in the datasets are categorical. And categorical data are not readable for the algorithms we select. We prefer to use one-hot encoding technique to convert the characterized feature matrix into vectorized feature space.
#### Data Normalization
Most machine learning methods are more powerful when the data are scaled into one uniform range. We will normalize the features using StandardScaler provided by Spark to make each feature scale to unit standard deviation.
#### Training and Test set separation
After data normalization is done, we are going to separate the whole data set into training and test set respectively. The ratio between the training and test data is 80%: 20%.
#### Data balance 
Since we get the training set, we will observe the ratio of different labeled instances to see if there is a class imbalance in the training set. we will handle this issue by down-sampling the major class and make the ratio of two classes as 50%: 50%.
### Training
For the training part, we are going to build a KNN classifier, a Random Forest classifier and Naive Bayes classifier based on the training set by using a hyperparameter search with 5-fold cross-validation techniques to find the best hyperparameters for building the model. Besides, based on common sense, the penalty of “failing to detect a poisonous mushroom” should be much higher than “failing to recognize an edible mushroom”. In other words, it is more important to detect a poisonous mushroom. So we will use a customized cost matrix to help build the classifiers as well.
### Evaluation 
To evaluate the results of models, precision, recall, F1 score and accuracy will be used as the main standard. We will compare the two classifiers with those scores by testing the test set to see which one is better. 
We will also compare the time cost of training the classifiers to achieve better performance. 
## Reference
[1] Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.<br/>
[2] Lima, A. D., Fortes, R. C., Novaes, M. G., & Percário, S. (2012). Poisonous mushrooms; a review of the most common intoxications. Nutricion hospitalaria, 27(2), 402-408.<br/>
[3] Verma, S. K., & Dutta, M. (2018). Mushroom classification using ANN and ANFIS algorithm. IOSR Journal of Engineering (IOSRJEN), 8(01), 94-100.<br/>
[4] Husaini, Muhammad. "A Data Mining Based On Ensemble Classifier Classification Approach for Edible Mushroom Identification." (2018).
